# Performance Testing with Locust

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Locust](https://img.shields.io/badge/Locust-00B140?style=for-the-badge)
![Status](https://img.shields.io/badge/Status-Complete-success?style=for-the-badge)

> Advanced load testing framework with multi-endpoint coverage, SLA assertions, and interactive analytics dashboard

---

## ğŸ¯ Project Overview

This project demonstrates **professional performance testing** using Locust to analyze API behavior under load. It includes automated test execution, SLA validation, custom metrics tracking, and visual analytics.

**API Under Test:** [JSONPlaceholder](https://jsonplaceholder.typicode.com) - REST API for testing and prototyping

**Key Features:**

- âœ… 10 endpoint coverage (GET, POST, PUT operations)
- âœ… 3 load scenarios (10, 50, 100 concurrent users)
- âœ… SLA assertions with real-time violation tracking
- âœ… Automated analysis with visual dashboard
- âœ… Event hooks for lifecycle management
- âœ… Zero-failure stability testing

---

## ğŸ“Š Quick Results

| Metric                | 10 Users | 50 Users | 100 Users | Status       |
| --------------------- | -------- | -------- | --------- | ------------ |
| **Total Requests**    | 267      | 998      | 905       | âš ï¸ Decreased |
| **Avg Response Time** | 93ms     | 267ms    | 1,122ms   | ğŸ”´ +1,107%   |
| **RPS**               | 4.63     | 16.89    | 15.38     | ğŸ”´ Collapsed |
| **Failures**          | 0        | 0        | 0         | âœ… Stable    |

**Critical Finding:** System degrades severely at 100 users but maintains zero failure rate (graceful degradation).

ğŸ“ˆ [View Full Analysis](ANALYSIS.md) | ğŸ“Š [Endpoint Comparison](reports/COMPARISON.md) | ğŸ¨ [Interactive Dashboard](reports/dashboard.html)

---

## ğŸ—‚ï¸ Project Structure

```
performance-testing-locust/
â”œâ”€â”€ locustfile.py              # Main test scenarios (10 endpoints)
â”œâ”€â”€ config.py                  # SLA thresholds and configuration
â”œâ”€â”€ run_tests.bat              # Automated test suite (Windows)
â”œâ”€â”€ run_tests.sh               # Automated test suite (Linux/Mac)
â”œâ”€â”€ analyze_results.py         # CSV analysis and comparison generator
â”œâ”€â”€ generate_charts.py         # Interactive dashboard generator
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ README.md                  # This file
â”œâ”€â”€ ANALYSIS.md                # Deep-dive performance analysis
â”œâ”€â”€ RESULTS.md                 # Detailed test results
â”œâ”€â”€ METRICS.md                 # Custom metrics documentation
â””â”€â”€ reports/                   # Generated test reports
    â”œâ”€â”€ dashboard.html         # Interactive visual dashboard
    â”œâ”€â”€ COMPARISON.md          # Endpoint-by-endpoint comparison
    â”œâ”€â”€ report_*_*.html        # Locust HTML reports (timestamped)
    â””â”€â”€ results_*_*.csv        # Raw test data (timestamped)
```

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- pip
- Virtual environment (recommended)

### Installation

```bash
# Clone repository
git clone https://github.com/ChandeDeVargas/performance-testing-locust.git
cd performance-testing-locust

# Create virtual environment
python -m venv venv

# Activate virtual environment
# Windows:
venv\Scripts\activate
# Linux/Mac:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

---

## â–¶ï¸ Running Tests

### Option 1: Automated Test Suite (Recommended)

Run all 3 load scenarios with a single command:

```bash
# Windows
run_tests.bat

# Linux/Mac
./run_tests.sh
```

**What it does:**

- âœ… Auto-detects virtual environment (no manual activation needed)
- âœ… Executes 3 scenarios: 10, 50, 100 concurrent users
- âœ… Generates timestamped HTML and CSV reports
- âœ… Validates SLA compliance in real-time
- âœ… Logs slow requests and violations

**Duration:** ~3-4 minutes total (60s per scenario + processing)

---

### Option 2: Manual Execution

#### Baseline Test (10 users - Normal Load)

```bash
locust -f locustfile.py --headless -u 10 -r 2 -t 60s --stop-timeout 10 \
  --html reports/baseline.html --csv reports/baseline
```

#### Medium Load Test (50 users - Moderate Stress)

```bash
locust -f locustfile.py --headless -u 50 -r 5 -t 60s --stop-timeout 10 \
  --html reports/medium.html --csv reports/medium
```

#### Stress Test (100 users - High Load)

```bash
locust -f locustfile.py --headless -u 100 -r 10 -t 60s --stop-timeout 10 \
  --html reports/stress.html --csv reports/stress
```

---

### Option 3: Interactive Web UI

For real-time monitoring and manual control:

```bash
locust -f locustfile.py
```

Then open: **http://localhost:8089**

Configure users, spawn rate, and duration in the browser interface.

---

## ğŸ“ˆ Analysis & Visualization

### Generate Reports

After running tests, generate comprehensive analysis:

```bash
# 1. Generate endpoint comparison (Markdown report)
python analyze_results.py

# 2. Generate interactive visual dashboard (HTML charts)
python generate_charts.py
```

### View Results

**Interactive Dashboard:**

```bash
cd reports
python -m http.server 8000
```

Open in browser: **http://localhost:8000/dashboard.html**

**Features:**

- ğŸ“Š Bar chart: Response time by load level
- ğŸ“ˆ Line chart: Performance degradation trends
- ğŸ“‹ Summary statistics cards
- ğŸ¨ Beautiful gradient design

**Static Reports:**

- `reports/COMPARISON.md` - Detailed endpoint comparison
- `ANALYSIS.md` - Comprehensive performance analysis
- `RESULTS.md` - Test results with insights
- `METRICS.md` - Custom metrics documentation

---

## ğŸ¯ Test Coverage

### Endpoints Tested (10 Total)

**Posts Resource:**

- `GET /posts` - Browse all posts (SLA: <500ms)
- `GET /posts/1` - View single post (SLA: <300ms)
- `GET /posts?userId=1` - Filtered posts (SLA: <600ms)
- `POST /posts` - Create new post (SLA: <1000ms)
- `PUT /posts/1` - Update post (SLA: <800ms)

**Users Resource:**

- `GET /users` - Browse all users (SLA: <500ms)
- `GET /users/1` - View single user (SLA: <300ms)

**Other Resources:**

- `GET /comments` - Browse comments (SLA: <400ms)
- `GET /albums` - Browse albums (SLA: <500ms)

### Load Scenarios

| Scenario     | Users | Spawn Rate | Duration | Purpose                  |
| ------------ | ----- | ---------- | -------- | ------------------------ |
| **Baseline** | 10    | 2/sec      | 60s      | Normal user behavior     |
| **Medium**   | 50    | 5/sec      | 60s      | Moderate traffic spike   |
| **Stress**   | 100   | 10/sec     | 60s      | Peak load / Black Friday |

---

## ğŸ”¬ Advanced Features

### 1. SLA Assertions âœ…

Every request is validated against response time limits defined in `config.py`:

```python
SLA_THRESHOLDS = {
    "GET": {
        "/posts": 500,      # Max 500ms
        "/posts/1": 300,    # Max 300ms
        # ... more endpoints
    }
}
```

**Real-time Validation:**

- SLA violations logged immediately
- Total violations reported at test end
- Enables performance regression detection

---

### 2. Custom Metrics ğŸ“Š

**Tracked Automatically:**

- âš ï¸ Slow requests (>2 seconds)
- ğŸ”´ SLA violations per endpoint
- ğŸ“ˆ Response time percentiles (p50, p95, p99)
- ğŸ¯ Request distribution by endpoint

**Event Hooks:**

```python
on_test_start()  â†’ Initialize logging, display config
on_request()     â†’ Validate SLA, track slow requests
on_test_stop()   â†’ Summary statistics, top 5 slowest
```

---

### 3. Multi-Layer Validation âœ…

Each request goes through:

1. **Status Code Check** - Expected vs actual (200, 201, etc.)
2. **Response Structure** - Required fields present
3. **Response Content** - Non-empty, valid JSON
4. **SLA Compliance** - Response time within limits

---

### 4. Automated Analysis ğŸ¤–

**CSV Parser (`analyze_results.py`):**

- Finds latest test results automatically
- Parses all endpoint statistics
- Calculates performance degradation
- Generates comparison tables

**Chart Generator (`generate_charts.py`):**

- Creates interactive Chart.js visualizations
- Bar charts for response time comparison
- Line charts for degradation trends
- Summary statistics cards

---

## ğŸ“š Documentation

### Analysis Documents

| File                                           | Description                                                                |
| ---------------------------------------------- | -------------------------------------------------------------------------- |
| [ANALYSIS.md](ANALYSIS.md)                     | Deep-dive performance analysis, bottleneck identification, recommendations |
| [RESULTS.md](RESULTS.md)                       | Detailed test results with metrics tables                                  |
| [METRICS.md](METRICS.md)                       | Custom metrics implementation and SLA definitions                          |
| [reports/COMPARISON.md](reports/COMPARISON.md) | Auto-generated endpoint comparison                                         |

### Key Findings

**Performance Degradation:**

- Worst endpoint: `GET /users/1` (+1,796% at 100 users)
- Best endpoint: `GET /users` (+515% at 100 users)
- Average degradation: +1,107% across all endpoints

**Bottleneck Analysis:**

- ğŸ”´ Database connection pool exhaustion
- ğŸ”´ Single-threaded request processing
- ğŸ”´ No HTTP keep-alive / connection pooling
- ğŸ”´ Lack of caching layer

**Recommendations:**

1. Set production limit: 50 concurrent users max
2. Implement Redis caching layer
3. Increase database connection pool
4. Enable async request processing

See [ANALYSIS.md](ANALYSIS.md) for full details.

---

## ğŸ› ï¸ Configuration

### SLA Thresholds (`config.py`)

Customize response time limits per endpoint:

```python
SLA_THRESHOLDS = {
    "GET": {
        "/posts": 500,      # Milliseconds
        "/users": 500,
        # ... customize as needed
    }
}
```

### Feature Flags

Enable/disable features:

```python
ENABLE_ASSERTIONS = True        # Fail tests on SLA violations
ENABLE_DETAILED_LOGGING = True  # Log each request details
ENABLE_PERCENTILE_TRACKING = True
```

---

## ğŸ“ What This Project Demonstrates

### Performance Testing Skills:

- âœ… Load test design with realistic user behavior
- âœ… Metrics analysis (RPS, response times, percentiles)
- âœ… Bottleneck identification and root cause analysis
- âœ… SLA definition and compliance tracking
- âœ… Report generation and data visualization

### Technical Skills:

- âœ… Locust framework (Python)
- âœ… HTTP/REST API testing
- âœ… CLI automation (headless mode)
- âœ… Event-driven architecture (hooks)
- âœ… Data parsing and analysis (CSV)
- âœ… Interactive visualizations (Chart.js)
- âœ… Shell scripting (Bash, Batch)
- âœ… Git version control

### Professional Practices:

- âœ… Modular code organization
- âœ… Configuration management
- âœ… Automated testing pipelines
- âœ… Comprehensive documentation
- âœ… Reproducible results

---

## ğŸ” Viewing Reports

### HTML Reports (Locust Native)

```bash
# From reports directory
cd reports
python -m http.server 8000
```

Open: `http://localhost:8000/report_10users_YYYYMMDD_HHMMSS.html`

**Tabs:**

- **Statistics:** Request counts, failures, response times, RPS
- **Charts:** Response time percentiles, RPS over time
- **Failures:** Error details (if any)
- **Exceptions:** Stack traces (if any)

### Dashboard (Custom Visualizations)

```bash
cd reports
python -m http.server 8000
```

Open: `http://localhost:8000/dashboard.html`

**Features:**

- Interactive bar charts (response time by load)
- Performance trend lines
- Summary statistics cards
- Responsive design

âš ï¸ **Note:** Do NOT use Live Server (VSCode extension) - use Python HTTP server for correct rendering.

---

## ğŸ“¦ Dependencies

```txt
locust==2.32.4
```

**Additional Requirements:**

- Python 3.8+
- Modern web browser (for dashboard viewing)

---

## ğŸš¦ CI/CD Integration (Future)

Example GitHub Actions workflow:

```yaml
name: Performance Tests

on: [push, pull_request]

jobs:
  performance:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: "3.10"
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
      - name: Run performance tests
        run: |
          ./run_tests.sh
      - name: Upload reports
        uses: actions/upload-artifact@v2
        with:
          name: performance-reports
          path: reports/
```

---

## ğŸ¤ Contributing

This is a personal portfolio project, but suggestions are welcome!

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

---

## ğŸ“„ License

MIT License - feel free to use this project as a template for your own performance testing needs.

---

## ğŸ‘¤ Author

**Chande De Vargas**

- GitHub: [@ChandeDeVargas](https://github.com/ChandeDeVargas)
- LinkedIn: [Chande De Vargas](https://www.linkedin.com/in/chande-de-vargas-b8a51838a/)
- Portfolio: Performance Testing Specialist

---

## ğŸ“š Resources & References

- [Locust Documentation](https://docs.locust.io/)
- [JSONPlaceholder API](https://jsonplaceholder.typicode.com/)
- [Performance Testing Best Practices](https://martinfowler.com/articles/practical-test-pyramid.html)
- [HTTP Load Testing](https://www.w3.org/Protocols/rfc2616/rfc2616.html)

---

## ğŸ™ Acknowledgments

- Locust.io team for the excellent load testing framework
- JSONPlaceholder for providing a reliable test API
- Chart.js for beautiful visualizations

---

**â­ If you found this project helpful, please consider giving it a star!**
